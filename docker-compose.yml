services:
  open-meteo:
    image: public.ecr.aws/w5w8t1y7/openmeteo:latest
    container_name: upgrade_open_meteo
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/forecast?latitude=47&longitude=28&current_weather=true"]
      interval: 30s
      timeout: 10s
      retries: 3
    ports:
      - "0.0.0.0:8080:8080"
    volumes:
      - ./open-meteo/config:/app/config
      - ./open-meteo/data:/app/data
    networks:
      - upgrade_network

  postgres:
    image: postgis/postgis:15-3.3
    container_name: upgrade_postgres
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/migrations:/docker-entrypoint-initdb.d/
    ports:
      - "0.0.0.0:5432:5432"
    networks:
      - upgrade_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 30s
      timeout: 10s
      retries: 3

  pgadmin:
    image: dpage/pgadmin4:8.0 # registry.redhat.io/rhel8/pgadmin4:latest
    container_name: upgrade_pgadmin
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@upgrade.com
      - PGADMIN_DEFAULT_PASSWORD=admin123
    ports:
      - "0.0.0.0:5050:80"
    networks:
      - upgrade_network
    depends_on:
      - postgres
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: upgrade_redis
    command: redis-server --requirepass ${REDIS_PASSWORD}
    ports:
      - "0.0.0.0:6379:6379"
    networks:
      - upgrade_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  minio:
    image: minio/minio:latest
    container_name: upgrade_minio
    networks: [upgrade_network]
    ports:
      - "0.0.0.0:9000:9000"
      - "0.0.0.0:9001:9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - ./data/minio:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3

  airflow-webserver:
    image: apache/airflow:2.7.3-python3.11
    container_name: upgrade_airflow_webserver
    command: webserver
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID}
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=${AIRFLOW__WEBSERVER__EXPOSE_CONFIG}
    volumes:
      - ../../airflow/dags:/opt/airflow/dags
      - ../../airflow/plugins:/opt/airflow/plugins
      - ../../airflow/config:/opt/airflow/config
      - airflow_logs:/opt/airflow/logs
    ports:
      - "0.0.0.0:8081:8080"
    networks:
      - upgrade_network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  airflow-scheduler:
    image: apache/airflow:2.7.3-python3.11
    container_name: upgrade_airflow_scheduler
    command: scheduler
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID}
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
    volumes:
      - ../../airflow/dags:/opt/airflow/dags
      - ../../airflow/plugins:/opt/airflow/plugins
      - ../../airflow/config:/opt/airflow/config
      - airflow_logs:/opt/airflow/logs
    networks:
      - upgrade_network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped

  airflow-init:
    image: apache/airflow:2.7.3-python3.11
    container_name: upgrade_airflow_init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo "Setting AIRFLOW_UID to $(id -u)"
          export AIRFLOW_UID=$(id -u)
        fi
        
        echo "Creating directories..."
        mkdir -p /opt/airflow/{logs,dags,plugins,config}
        
        echo "Setting permissions..."
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/
        
        echo "Initializing database..."
        /entrypoint airflow db init
        
        echo "Creating admin user..."
        /entrypoint airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@upgrade.com \
          --password admin123
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID}
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - _AIRFLOW_DB_MIGRATE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin123
    volumes:
      - airflow_logs:/opt/airflow/logs
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
    networks:
      - upgrade_network
    depends_on:
      postgres:
        condition: service_healthy
    restart: "no"
    user: "0:0"

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: upgrade_zookeeper
    healthcheck:
      test: ["CMD", "bash", "-c", "echo 'ruok' | nc localhost 2181"]
      interval: 30s
      timeout: 10s
      retries: 3
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
    networks:
      - upgrade_network

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: upgrade_kafka
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - zookeeper
    ports:
      - "0.0.0.0:9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 168
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - upgrade_network

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: upgrade_kafka_ui
    ports:
      - "0.0.0.0:8082:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
    networks:
      - upgrade_network
    depends_on:
      - kafka

  weather-producer:
    build:
      context: ./kafka/producer
      dockerfile: Dockerfile
    container_name: upgrade_weather_producer
    healthcheck:
      test: ["CMD", "pgrep", "-f", "weather_producer"]
      interval: 60s
      timeout: 10s
      retries: 3
    depends_on:
      - kafka
      - open-meteo
      - redis
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      # OPEN_METEO_URL: http://open-meteo:8080/v1/forecast  # Ваш локальный API
      OPEN_METEO_URL: https://api.open-meteo.com/v1/forecast
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      COLLECTION_INTERVAL_MINUTES: 30
      LOG_LEVEL: INFO
    volumes:
      - ./logs:/app/logs
    networks:
      - upgrade_network
    restart: unless-stopped

  # Weather Consumer - сохраняет в PostgreSQL и MinIO
  weather-consumer:
    build:
      context: ./kafka/consumer
      dockerfile: Dockerfile
    container_name: upgrade_weather_consumer
    healthcheck:
      test: ["CMD", "pgrep", "-f", "weather_consumer"]
      interval: 60s
      timeout: 10s
      retries: 3
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      POSTGRES_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD}
      MINIO_BUCKET: weather-data
      LOG_LEVEL: INFO
    volumes:
      - ./logs:/app/logs
    networks:
      - upgrade_network
    restart: unless-stopped

  streamlit-app:
    build:
      context: ./streamlit
      dockerfile: Dockerfile
    container_name: upgrade_streamlit
    ports:
      - "0.0.0.0:8501:8501"
    volumes:
      - ./data:/data
      - ./results:/results
    environment:
      # Исправленные переменные окружения
      - POSTGRES_HOST=postgres
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_PORT=5432
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}  # Добавлено
      
      # Streamlit конфигурация
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      - STREAMLIT_SERVER_HEADLESS=true
      - STREAMLIT_BROWSER_GATHER_USAGE_STATS=false
    
    networks:
      - upgrade_network
    depends_on:
      postgres:
        condition: service_healthy  # Ждем пока Postgres будет готов
      redis:
        condition: service_healthy  # Ждем пока Redis будет готов
    restart: unless-stopped
    
    # Добавим healthcheck для Streamlit
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/healthz"] 
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
  
  # nextflow-worker:
  #   image: nextflow/nextflow:23.10.0
  #   container_name: upgrade_nextflow_worker
  #   restart: unless-stopped
  #   volumes:
  #     - ./nextflow:/pipeline:ro
  #     - ./data:/data
  #     - ./results:/results
  #     - /var/run/docker.sock:/var/run/docker.sock
  #     - nextflow_work:/work
  #   environment:
  #     - NXF_WORK=/work
  #     - NXF_ASSETS=/pipeline/assets
  #     - NXF_TEMP=/tmp
  #     - NXF_HOME=/pipeline

  #     # Database connection
  #     - POSTGRES_HOST=postgres
  #     - POSTGRES_DB=upgrade_db
  #     - POSTGRES_USER=upgrade_user
  #     - POSTGRES_PASSWORD=upgrade_password
      
  #     # MinIO connection
  #     - MINIO_ENDPOINT=http://minio:9000
  #     - MINIO_ACCESS_KEY=minioadmin
  #     - MINIO_SECRET_KEY=minioadmin
      
  #     # Redis для job queue
  #     - REDIS_HOST=redis
  #     - REDIS_PORT=6379
      
  #     # Resource limits
  #     - NXF_OPTS='-Xms1g -Xmx4g'
    
  #   working_dir: /pipeline
  #   networks:
  #     - upgrade_network
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #     redis:
  #       condition: service_started
  #     minio:
  #       condition: service_healthy
  #   # Команда для job worker
  #   command: python3 bin/nextflow_job_worker.py
  #   healthcheck:
  #     test: ["CMD", "python3", "bin/health_check.py"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  # # Nextflow Job Scheduler (опционально - для автоматического scheduling)
  # nextflow-scheduler:
    # image: nextflow/nextflow:23.10.0
    # container_name: nextflow-scheduler
    # restart: unless-stopped
    # profiles: ["scheduler"]  # Запускается только при необходимости
    # volumes:
    #   - ./nextflow-pipeline:/pipeline:ro
    #   - ./data:/data:ro
    # environment:
    #   - POSTGRES_HOST=postgres
    #   - POSTGRES_DB=upgrade_db  
    #   - POSTGRES_USER=upgrade_user
    #   - POSTGRES_PASSWORD=upgrade_password
    #   - REDIS_HOST=redis
    # working_dir: /pipeline
    # networks:
    #   - upgrade-network
    # depends_on:
    #   - postgres
    #   - redis
    # command: python3 bin/schedule_genomics_jobs.py

networks:
  upgrade_network:
    driver: bridge
    name: upgrade_network

volumes:
  postgres_data:
    name: upgrade_postgres_data
  minio_data:
    name: upgrade_minio_data
  airflow_logs:
    name: upgrade_airflow_logs
  kafka_data:
    name: upgrade_kafka_data
  zookeeper_data:
    name: upgrade_zookeeper_data